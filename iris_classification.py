# -*- coding: utf-8 -*-
"""Iris classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RI5w79jYinFGrJyZa38yIRLdZBgD_uT5

Importing Dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""Loading the Data"""

df = pd.read_csv('/content/Iris.csv')

"""Preview Data"""

df.head()

"""Data Information"""

df.info()

"""Statistics"""

df.describe()

"""Duplicates"""

df.duplicated().sum()

"""Missing values"""

df.isnull().sum()

"""Class Distribution"""

df['Species'].value_counts()

"""Pair plot"""

sns.pairplot(df, hue='Species')

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

for species in df['Species'].unique():
    species_data = df[df['Species'] == species]
    ax.scatter(species_data['SepalLengthCm'], species_data['SepalWidthCm'], species_data['PetalLengthCm'],
               label=species)

ax.set_xlabel('Sepal Length (cm)')
ax.set_ylabel('Sepal Width (cm)')
ax.set_zlabel('Petal Length (cm)')
ax.legend()
plt.title('3D Scatter Plot of Iris Dataset')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Create bins for Sepal Length
bins = np.linspace(df['SepalLengthCm'].min(), df['SepalLengthCm'].max(), 5)

# Group data by bins and species
grouped = df.groupby([pd.cut(df['SepalLengthCm'], bins), 'Species'])['Species'].count().unstack()

# Plot stacked bar chart
grouped.plot(kind='bar', stacked=True)
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Count')
plt.title('Stacked Bar Chart of Species by Sepal Length')
plt.legend(title='Species')
plt.show()

"""Feature Split"""

# Separate features and target
data = df.values
X = data[:,0:4]
Y = data[:,4]

# Calculate average of each features for all classes
Y_Data = np.array([np.average(X[:, i][Y==j].astype('float32')) for i in range (X.shape[1])
 for j in np.unique(Y)])
# the size of Y_Data should be 12, however it's 88
# This might happen because np.unique(Y) is not what's expected
# Let's check the unique values in Y to see if it matches our expectation
print(np.unique(Y))
# adjust the Y_Data calculation based on the actual unique values of Y
# e.g., if np.unique(Y) returns 3 values, you can modify the code as:
Y_Data = np.array([np.average(X[:, i][Y==j].astype('float32')) for i in range (X.shape[1])
 for j in np.unique(Y)[:3]]) # select the first 3 unique values

# now Y_Data should have the expected size of 12, and you can reshape it
Y_Data_reshaped = Y_Data.reshape(4, 3)
Y_Data_reshaped = np.swapaxes(Y_Data_reshaped, 0, 1)
X_axis = np.arange(len(df.columns)-1) # Assuming you want to exclude 'Species' column
width = 0.25

"""Averages"""

# Plot the average
# X_axis is changed to have the correct number of elements (4)
X_axis = np.arange(len(df.columns) - 2)  # Exclude 'Id' and 'Species' columns for 4 features
plt.bar(X_axis, Y_Data_reshaped[0], width, label='Setosa')
plt.bar(X_axis + width, Y_Data_reshaped[1], width, label='Versicolour')
plt.bar(X_axis + width * 2, Y_Data_reshaped[2], width, label='Virginica')
plt.xticks(X_axis, df.columns[1:5])  # Set x-axis labels to feature names, excluding 'Id' and 'Species'
plt.xlabel("Features")
plt.ylabel("Value in cm.")
plt.legend(bbox_to_anchor=(1.3, 1))
plt.show()

# Split dataset into features (X) and target (y)
X = df.drop(columns=['Species'])  # Features
y = df['Species']  # Target variable

"""Train-Test-Split"""

from sklearn.model_selection import train_test_split
# Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Label Encoding"""

from sklearn.preprocessing import LabelEncoder

# Example: Encoding the 'Species' column
le = LabelEncoder()
df['Species'] = le.fit_transform(df['Species'])

# Check encoding
print(df['Species'].unique())

df.head()

"""Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report # Import accuracy_score and classification_report
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

"""Support Vector Classifer"""

from sklearn.svm import SVC # Import the SVC class from sklearn.svm

svm = SVC(kernel='linear')  # Linear Kernel
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))

"""Checking Accuracy"""

print(f"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}")

print(svm)  # Check model details
print("Classes:", np.unique(y_train))  # Ensure all classes are present in training data

print("Class distribution in training set:")
print(pd.Series(y_train).value_counts())

# Get feature importance
feature_importance = rf.feature_importances_

# Define feature_columns using the DataFrame columns
feature_columns = df.columns[:-1]  # All columns except 'Species'

# Print feature importance for each feature
for i, feature in enumerate(feature_columns):
    print(f"{feature}: {feature_importance[i]}")

"""Prediction"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
import pandas as pd
# Get a data point for Iris-versicolor
versicolor_data = df[df['Species'] == 1].iloc[0, :-1].values  # Exclude 'Species' column, use encoded value
# Convert it to the format expected by your model and make predictions
X_new = versicolor_data.reshape(1, -1)  # Reshape to (1, number_of_features)

# Assuming X_train, y_train, df, and X_new are already defined

# Standardize the data
# Include all feature columns during scaling
feature_columns = df.columns[:-1]  # All columns except 'Species'
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train[feature_columns])

# Create DataFrame for new data with the correct columns
X_new_df = pd.DataFrame(X_new, columns=feature_columns)

# Transform new data using the trained scaler
X_new_scaled = scaler.transform(X_new_df)

# Train the Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)

# Predict on new data
prediction = rf.predict(X_new_scaled)
print("Random Forest Prediction of Species:", prediction)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you want to use the Random Forest predictions:
cm = confusion_matrix(y_test, y_pred_rf)  # Changed y_pred to y_pred_rf

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# **Conclusion**

The Iris Flower Classification project helped us
understand how machine learning models can be trained to classify species based on flower measurements. Here's what we learned and extracted:

**1**.Cleaned and explored the dataset, ensuring there were no missing or duplicate values.

**2**.Visualized relationships between features (like petal length and width) and species.

**3**.Preprocessed the data by encoding categorical labels and splitting into train-test sets.

**4**.Trained models like Random Forest and SVM to classify flower species.

**5**.Achieved high accuracy in predictions, showing that the features are strong indicators of the species.

**6**.Understood the importance of model evaluation using metrics like accuracy and classification report.

**7**.Overall, this project gave practical experience with data cleaning, visualization, feature extraction, and model buildingâ€”key skills for a Data Analyst or ML beginner.
"""